# Quick Start Guide

## ğŸš€ One Command to Rule Them All

```powershell
start.bat
```

This single command will:
1. âœ… Start Spark Master and Worker
2. âœ… Start Elasticsearch for metrics storage
3. âœ… Start Kibana with pre-configured dashboard
4. âœ… Start OpenTelemetry Collector
5. âœ… Automatically run the Spark application
6. âœ… Display all access URLs

## â±ï¸ Timeline

- **0-30 seconds**: Docker services starting
- **30-40 seconds**: Spark application starting
- **40-120 seconds**: Metrics flowing to Elasticsearch
- **2+ minutes**: Dashboard fully populated

## ğŸ¯ Access Points

| Service | URL | Purpose |
|---------|-----|---------|
| **Kibana Dashboard** | http://localhost:5601 | Main monitoring interface |
| Spark Master | http://localhost:8080 | Cluster overview |
| Spark Worker | http://localhost:8081 | Worker details |
| Spark Application | http://localhost:4040 | Running job details |
| Elasticsearch | http://localhost:9200 | Metrics API |

## ğŸ“Š View the Dashboard

1. Open http://localhost:5601
2. Click **Analytics** â†’ **Dashboard**
3. Select **"Spark Cluster Monitoring"**
4. Watch metrics update every 30 seconds

## ğŸ§ª Test Everything

```powershell
test.bat
```

Runs 8 automated tests to verify:
- All services running
- Metrics flowing to Elasticsearch
- Dashboard available in Kibana
- Spark UIs accessible
- Application running

## ğŸ›‘ Stop Everything

```powershell
stop.bat
```

## ğŸ”„ Change Workload Type

Want to test different Spark patterns?

```powershell
# Memory-intensive workload
run-app.bat memory 5

# Slow task simulation
run-app.bat slow 5

# Skewed data distribution
run-app.bat skewed 5

# Failure scenarios
run-app.bat failure 5
```

## ğŸ“ Project Files

```
Essential files:
â”œâ”€â”€ start.bat              â† Start everything
â”œâ”€â”€ stop.bat               â† Stop everything
â”œâ”€â”€ run-app.bat            â† Alternative workloads
â”œâ”€â”€ docker-compose.yml     â† Service definitions
â”œâ”€â”€ README.md              â† Full documentation
â”œâ”€â”€ otel/
â”‚   â””â”€â”€ otel-collector-config.yaml
â”œâ”€â”€ kibana/dashboards/
â”‚   â””â”€â”€ spark-dashboard-v9.ndjson
â””â”€â”€ spark/apps/
    â””â”€â”€ anomaly_detection_app.py
```

## â“ Quick Troubleshooting

**No metrics showing?**
- Wait 2 minutes
- Check http://localhost:4040 is accessible
- Run: `docker logs otel-collector --tail 50`

**Services won't start?**
- Ensure Docker Desktop is running
- Run: `stop.bat` then `start.bat`

**Dashboard not found?**
- Check http://localhost:5601/app/dashboards
- Look for "Spark Cluster Monitoring"

## ğŸ¯ What You're Monitoring

The dashboard shows:
- âœ… Active Spark applications
- âœ… Stage completion statistics
- âœ… I/O operations (read/write records)
- âœ… Shuffle operations
- âœ… Real-time activity trends

## ğŸ“– Need More Details?

See `README.md` for:
- Architecture details
- Configuration options
- Advanced troubleshooting
- Customization guide

---

**That's it! You're now monitoring Spark with one command! ğŸ‰**
